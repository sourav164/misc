{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bea7325",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, MaxPooling1D, Flatten, Dense\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      7\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c24b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "     ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.4/9.3 MB 9.2 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 1.8/9.3 MB 18.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 3.3/9.3 MB 23.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 5.2/9.3 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 7.1/9.3 MB 30.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 8.8/9.3 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 9.3/9.3 MB 28.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\sr\\miniconda3\\envs\\cnn\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 302.2/302.2 kB 18.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\sr\\miniconda3\\envs\\cnn\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.2 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3267e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Concrete_Data.csv\")\n",
    "y = df[\"Con_com\"]\n",
    "x = df.iloc[:, :-1]\n",
    "X_train, X_test1, y_train, y_test1  = train_test_split(x, y,test_size=0.3, random_state=420, shuffle=True)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test1, y_test1,test_size=0.5, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(20, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer with 1 neuron for regression\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Mean Squared Error on Test Data: {loss}\")\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
