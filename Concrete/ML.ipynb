{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bea7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3267e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Concrete_Data.csv\")\n",
    "y = df[\"Con_com\"]\n",
    "x = df.iloc[:, :-1]\n",
    "X_train, X_test1, y_train, y_test1  = train_test_split(x, y,test_size=0.3, random_state=420, shuffle=True)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test1, y_test1,test_size=0.5, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f5cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy().reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.to_numpy().reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_valid = X_valid.to_numpy().reshape(X_valid.shape[0], X_valid.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cb3b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 579.7317 - val_loss: 260.6183\n",
      "Epoch 2/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 266.9380 - val_loss: 212.8328\n",
      "Epoch 3/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 212.3178 - val_loss: 179.5141\n",
      "Epoch 4/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 170.7840 - val_loss: 133.3715\n",
      "Epoch 5/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 138.9194 - val_loss: 133.3156\n",
      "Epoch 6/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 118.7789 - val_loss: 90.6402\n",
      "Epoch 7/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 95.0394 - val_loss: 83.2816\n",
      "Epoch 8/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 97.8679 - val_loss: 80.7101\n",
      "Epoch 9/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 97.3898 - val_loss: 160.0052\n",
      "Epoch 10/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 103.1777 - val_loss: 68.2782\n",
      "Epoch 11/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 73.9107 - val_loss: 58.8521\n",
      "Epoch 12/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 69.3607 - val_loss: 78.1978\n",
      "Epoch 13/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 67.8220 - val_loss: 54.0273\n",
      "Epoch 14/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 68.0740 - val_loss: 53.3550\n",
      "Epoch 15/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 64.0512 - val_loss: 59.2976\n",
      "Epoch 16/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 66.4808 - val_loss: 76.5113\n",
      "Epoch 17/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 84.8276 - val_loss: 63.3086\n",
      "Epoch 18/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 55.1297 - val_loss: 47.7149\n",
      "Epoch 19/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 58.0886 - val_loss: 71.7241\n",
      "Epoch 20/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 64.9343 - val_loss: 46.7085\n",
      "Epoch 21/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 56.9619 - val_loss: 53.8263\n",
      "Epoch 22/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 60.6173 - val_loss: 42.8710\n",
      "Epoch 23/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 65.9367 - val_loss: 48.8236\n",
      "Epoch 24/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 57.2157 - val_loss: 70.6654\n",
      "Epoch 25/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 55.8993 - val_loss: 41.6445\n",
      "Epoch 26/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 52.9638 - val_loss: 40.0012\n",
      "Epoch 27/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 63.4516 - val_loss: 40.6557\n",
      "Epoch 28/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 50.4083 - val_loss: 49.5174\n",
      "Epoch 29/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.0257 - val_loss: 48.0194\n",
      "Epoch 30/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 52.6006 - val_loss: 42.6774\n",
      "Epoch 31/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 47.7788 - val_loss: 42.7610\n",
      "Epoch 32/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 49.2932 - val_loss: 50.5837\n",
      "Epoch 33/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 54.1313 - val_loss: 44.2938\n",
      "Epoch 34/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.1990 - val_loss: 63.6886\n",
      "Epoch 35/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 63.5157 - val_loss: 68.2708\n",
      "Epoch 36/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 53.0245 - val_loss: 41.3129\n",
      "Epoch 37/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.1115 - val_loss: 40.6291\n",
      "Epoch 38/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 52.3877 - val_loss: 42.0788\n",
      "Epoch 39/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 48.5620 - val_loss: 52.8147\n",
      "Epoch 40/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 52.3161 - val_loss: 39.1049\n",
      "Epoch 41/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.9631 - val_loss: 47.8217\n",
      "Epoch 42/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 53.6417 - val_loss: 40.5697\n",
      "Epoch 43/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.3974 - val_loss: 44.0394\n",
      "Epoch 44/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 55.9650 - val_loss: 49.1197\n",
      "Epoch 45/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 56.1656 - val_loss: 39.0710\n",
      "Epoch 46/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 50.7879 - val_loss: 73.9647\n",
      "Epoch 47/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 49.9794 - val_loss: 39.0184\n",
      "Epoch 48/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 51.9953 - val_loss: 49.1737\n",
      "Epoch 49/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.3341 - val_loss: 66.3326\n",
      "Epoch 50/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 59.7850 - val_loss: 37.1116\n",
      "Epoch 51/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.0661 - val_loss: 38.9439\n",
      "Epoch 52/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 47.9625 - val_loss: 37.3057\n",
      "Epoch 53/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 49.6685 - val_loss: 70.0028\n",
      "Epoch 54/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 51.5844 - val_loss: 50.0223\n",
      "Epoch 55/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 49.7872 - val_loss: 51.4417\n",
      "Epoch 56/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.1312 - val_loss: 35.1684\n",
      "Epoch 57/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.6172 - val_loss: 39.5639\n",
      "Epoch 58/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 50.6162 - val_loss: 35.4548\n",
      "Epoch 59/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.6168 - val_loss: 39.6559\n",
      "Epoch 60/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.1816 - val_loss: 44.2386\n",
      "Epoch 61/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.5296 - val_loss: 36.6067\n",
      "Epoch 62/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 43.9503 - val_loss: 50.4813\n",
      "Epoch 63/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.7588 - val_loss: 37.2482\n",
      "Epoch 64/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.3886 - val_loss: 55.1498\n",
      "Epoch 65/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.0743 - val_loss: 61.3781\n",
      "Epoch 66/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 55.3426 - val_loss: 58.8303\n",
      "Epoch 67/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 52.0993 - val_loss: 49.3972\n",
      "Epoch 68/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 44.3815 - val_loss: 59.4216\n",
      "Epoch 69/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.5378 - val_loss: 37.7325\n",
      "Epoch 70/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.3027 - val_loss: 40.9969\n",
      "Epoch 71/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.4347 - val_loss: 40.2277\n",
      "Epoch 72/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.7216 - val_loss: 58.5181\n",
      "Epoch 73/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 47.7697 - val_loss: 41.4872\n",
      "Epoch 74/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 47.4958 - val_loss: 62.8816\n",
      "Epoch 75/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 49.1995 - val_loss: 43.8225\n",
      "Epoch 76/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.4217 - val_loss: 38.0093\n",
      "Epoch 77/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.3964 - val_loss: 75.2518\n",
      "Epoch 78/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.0778 - val_loss: 39.0478\n",
      "Epoch 79/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.2204 - val_loss: 35.1634\n",
      "Epoch 80/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.5334 - val_loss: 34.5657\n",
      "Epoch 81/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 2ms/step - loss: 45.2517 - val_loss: 47.3529\n",
      "Epoch 82/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.2184 - val_loss: 44.4020\n",
      "Epoch 83/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.1584 - val_loss: 38.6370\n",
      "Epoch 84/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.2813 - val_loss: 34.9298\n",
      "Epoch 85/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.0255 - val_loss: 51.4066\n",
      "Epoch 86/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.6685 - val_loss: 36.4217\n",
      "Epoch 87/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.0372 - val_loss: 43.6782\n",
      "Epoch 88/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 39.2816 - val_loss: 37.7378\n",
      "Epoch 89/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.4348 - val_loss: 40.1737\n",
      "Epoch 90/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.4736 - val_loss: 46.9351\n",
      "Epoch 91/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 43.9021 - val_loss: 40.0431\n",
      "Epoch 92/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.9927 - val_loss: 34.5045\n",
      "Epoch 93/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 55.7986 - val_loss: 36.3609\n",
      "Epoch 94/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.0663 - val_loss: 36.4229\n",
      "Epoch 95/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 47.1262 - val_loss: 62.5253\n",
      "Epoch 96/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 49.9784 - val_loss: 39.2521\n",
      "Epoch 97/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.0014 - val_loss: 37.2198\n",
      "Epoch 98/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.6023 - val_loss: 34.7258\n",
      "Epoch 99/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.7219 - val_loss: 39.8780\n",
      "Epoch 100/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.5950 - val_loss: 35.6836\n",
      "Epoch 101/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.0531 - val_loss: 36.2382\n",
      "Epoch 102/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.2486 - val_loss: 35.2333\n",
      "Epoch 103/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.1118 - val_loss: 37.7218\n",
      "Epoch 104/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.1210 - val_loss: 37.5759\n",
      "Epoch 105/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.9230 - val_loss: 37.0709\n",
      "Epoch 106/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.1706 - val_loss: 44.9238\n",
      "Epoch 107/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.8651 - val_loss: 34.3903\n",
      "Epoch 108/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.4426 - val_loss: 45.2448\n",
      "Epoch 109/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.1311 - val_loss: 90.1913\n",
      "Epoch 110/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 57.9537 - val_loss: 39.0384\n",
      "Epoch 111/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.7769 - val_loss: 40.4207\n",
      "Epoch 112/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.7921 - val_loss: 35.8316\n",
      "Epoch 113/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.7942 - val_loss: 37.6044\n",
      "Epoch 114/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 47.6657 - val_loss: 36.3224\n",
      "Epoch 115/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.6741 - val_loss: 36.2281\n",
      "Epoch 116/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.0767 - val_loss: 34.9521\n",
      "Epoch 117/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.0658 - val_loss: 35.3518\n",
      "Epoch 118/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 42.0551 - val_loss: 38.5392\n",
      "Epoch 119/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.6917 - val_loss: 66.6783\n",
      "Epoch 120/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.8962 - val_loss: 41.5787\n",
      "Epoch 121/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.4400 - val_loss: 45.0084\n",
      "Epoch 122/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.3536 - val_loss: 39.9206\n",
      "Epoch 123/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.2110 - val_loss: 44.2436\n",
      "Epoch 124/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.4147 - val_loss: 35.0966\n",
      "Epoch 125/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.9946 - val_loss: 34.3392\n",
      "Epoch 126/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.7366 - val_loss: 34.6555\n",
      "Epoch 127/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.8444 - val_loss: 38.3494\n",
      "Epoch 128/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.3850 - val_loss: 46.8522\n",
      "Epoch 129/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.3582 - val_loss: 42.0276\n",
      "Epoch 130/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.9942 - val_loss: 33.4694\n",
      "Epoch 131/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.3554 - val_loss: 33.5586\n",
      "Epoch 132/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.8137 - val_loss: 35.4359\n",
      "Epoch 133/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.1672 - val_loss: 37.1426\n",
      "Epoch 134/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.4269 - val_loss: 32.3533\n",
      "Epoch 135/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.6895 - val_loss: 35.1777\n",
      "Epoch 136/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.1062 - val_loss: 43.3747\n",
      "Epoch 137/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 40.4363 - val_loss: 34.3212\n",
      "Epoch 138/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 38.2901 - val_loss: 32.1380\n",
      "Epoch 139/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.4757 - val_loss: 35.2826\n",
      "Epoch 140/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.5035 - val_loss: 34.6645\n",
      "Epoch 141/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.6937 - val_loss: 68.9608\n",
      "Epoch 142/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.2037 - val_loss: 47.1128\n",
      "Epoch 143/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.1437 - val_loss: 32.4356\n",
      "Epoch 144/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 41.5807 - val_loss: 34.4647\n",
      "Epoch 145/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 36.9863 - val_loss: 32.9365\n",
      "Epoch 146/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.1965 - val_loss: 42.6756\n",
      "Epoch 147/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.2119 - val_loss: 44.6039\n",
      "Epoch 148/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.3474 - val_loss: 32.0193\n",
      "Epoch 149/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.2927 - val_loss: 32.8431\n",
      "Epoch 150/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.2338 - val_loss: 57.1451\n",
      "Epoch 151/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 47.7739 - val_loss: 35.8129\n",
      "Epoch 152/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.9578 - val_loss: 34.1206\n",
      "Epoch 153/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.3608 - val_loss: 38.1347\n",
      "Epoch 154/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 39.1056 - val_loss: 34.8995\n",
      "Epoch 155/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.3736 - val_loss: 33.9698\n",
      "Epoch 156/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 34.9911 - val_loss: 44.6960\n",
      "Epoch 157/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.4563 - val_loss: 30.6339\n",
      "Epoch 158/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 40.5019 - val_loss: 67.2300\n",
      "Epoch 159/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.0456 - val_loss: 35.8861\n",
      "Epoch 160/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.8426 - val_loss: 53.4083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.7641 - val_loss: 35.1579\n",
      "Epoch 162/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.8247 - val_loss: 46.5936\n",
      "Epoch 163/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.4199 - val_loss: 32.5832\n",
      "Epoch 164/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.3931 - val_loss: 37.7297\n",
      "Epoch 165/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.7616 - val_loss: 34.0412\n",
      "Epoch 166/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.3779 - val_loss: 35.6679\n",
      "Epoch 167/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.7845 - val_loss: 33.1686\n",
      "Epoch 168/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.4308 - val_loss: 34.0188\n",
      "Epoch 169/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.9271 - val_loss: 33.6738\n",
      "Epoch 170/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.6585 - val_loss: 41.5763\n",
      "Epoch 171/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.1174 - val_loss: 45.5691\n",
      "Epoch 172/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.1275 - val_loss: 31.0015\n",
      "Epoch 173/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.0150 - val_loss: 35.0368\n",
      "Epoch 174/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.0771 - val_loss: 30.0804\n",
      "Epoch 175/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.4045 - val_loss: 36.5899\n",
      "Epoch 176/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.3817 - val_loss: 34.8567\n",
      "Epoch 177/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.7928 - val_loss: 40.5127\n",
      "Epoch 178/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 53.8557 - val_loss: 43.1759\n",
      "Epoch 179/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.9061 - val_loss: 34.8157\n",
      "Epoch 180/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.4341 - val_loss: 35.9495\n",
      "Epoch 181/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.4107 - val_loss: 37.1656\n",
      "Epoch 182/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.7053 - val_loss: 33.0630\n",
      "Epoch 183/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.5466 - val_loss: 44.3098\n",
      "Epoch 184/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.5876 - val_loss: 37.3107\n",
      "Epoch 185/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.5382 - val_loss: 30.3936\n",
      "Epoch 186/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.4218 - val_loss: 30.8239\n",
      "Epoch 187/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.0931 - val_loss: 31.9856\n",
      "Epoch 188/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.7763 - val_loss: 39.0176\n",
      "Epoch 189/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.1796 - val_loss: 30.5102\n",
      "Epoch 190/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.7004 - val_loss: 45.7760\n",
      "Epoch 191/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.0905 - val_loss: 30.7185\n",
      "Epoch 192/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.4095 - val_loss: 34.8167\n",
      "Epoch 193/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.0519 - val_loss: 36.3080\n",
      "Epoch 194/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 34.1366 - val_loss: 28.1014\n",
      "Epoch 195/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.9826 - val_loss: 36.4590\n",
      "Epoch 196/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.2952 - val_loss: 32.6880\n",
      "Epoch 197/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.6746 - val_loss: 28.3676\n",
      "Epoch 198/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.6769 - val_loss: 51.4839\n",
      "Epoch 199/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.7071 - val_loss: 59.3946\n",
      "Epoch 200/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 43.1346 - val_loss: 29.9333\n",
      "Epoch 201/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.7809 - val_loss: 30.0953\n",
      "Epoch 202/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.7069 - val_loss: 29.8188\n",
      "Epoch 203/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.5028 - val_loss: 30.9475\n",
      "Epoch 204/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.1448 - val_loss: 27.6376\n",
      "Epoch 205/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.5341 - val_loss: 30.5853\n",
      "Epoch 206/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.7894 - val_loss: 30.7363\n",
      "Epoch 207/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.0668 - val_loss: 29.9592\n",
      "Epoch 208/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.4017 - val_loss: 52.1296\n",
      "Epoch 209/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.0503 - val_loss: 31.9712\n",
      "Epoch 210/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.7997 - val_loss: 31.5875\n",
      "Epoch 211/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.8458 - val_loss: 35.9753\n",
      "Epoch 212/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.3086 - val_loss: 30.3251\n",
      "Epoch 213/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.7678 - val_loss: 31.3882\n",
      "Epoch 214/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.5481 - val_loss: 27.4216\n",
      "Epoch 215/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.7787 - val_loss: 32.4967\n",
      "Epoch 216/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.6307 - val_loss: 29.7175\n",
      "Epoch 217/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.9877 - val_loss: 29.7653\n",
      "Epoch 218/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 29.7609 - val_loss: 27.1205\n",
      "Epoch 219/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.6903 - val_loss: 30.6063\n",
      "Epoch 220/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.8220 - val_loss: 31.9251\n",
      "Epoch 221/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.3607 - val_loss: 51.5626\n",
      "Epoch 222/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.0026 - val_loss: 30.0235\n",
      "Epoch 223/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.5167 - val_loss: 29.4573\n",
      "Epoch 224/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.9378 - val_loss: 45.5967\n",
      "Epoch 225/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.8398 - val_loss: 30.9102\n",
      "Epoch 226/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.3992 - val_loss: 32.4336\n",
      "Epoch 227/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.8069 - val_loss: 31.3774\n",
      "Epoch 228/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.3205 - val_loss: 50.9032\n",
      "Epoch 229/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.4923 - val_loss: 27.0329\n",
      "Epoch 230/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.4985 - val_loss: 27.9554\n",
      "Epoch 231/250\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 29.0631 - val_loss: 35.8776\n",
      "Epoch 232/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.2505 - val_loss: 34.9774\n",
      "Epoch 233/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.0938 - val_loss: 27.8092\n",
      "Epoch 234/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.6169 - val_loss: 27.5580\n",
      "Epoch 235/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.3798 - val_loss: 29.9425\n",
      "Epoch 236/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.1457 - val_loss: 30.5125\n",
      "Epoch 237/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.2060 - val_loss: 42.1467\n",
      "Epoch 238/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.4185 - val_loss: 31.1664\n",
      "Epoch 239/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.0505 - val_loss: 31.2643\n",
      "Epoch 240/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 1ms/step - loss: 30.9201 - val_loss: 29.7872\n",
      "Epoch 241/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.1990 - val_loss: 30.1440\n",
      "Epoch 242/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.7361 - val_loss: 31.8333\n",
      "Epoch 243/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.2321 - val_loss: 39.3418\n",
      "Epoch 244/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.7221 - val_loss: 33.6854\n",
      "Epoch 245/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.7243 - val_loss: 28.3087\n",
      "Epoch 246/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.2198 - val_loss: 32.3606\n",
      "Epoch 247/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.6677 - val_loss: 35.0638\n",
      "Epoch 248/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.3774 - val_loss: 29.9930\n",
      "Epoch 249/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.2460 - val_loss: 31.5052\n",
      "Epoch 250/250\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.6913 - val_loss: 34.8975\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 34.9584\n",
      "Mean Squared Error on Test Data: 34.95841598510742\n",
      "5/5 [==============================] - 0s 1000us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(8, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer with 1 neuron for regression\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=250, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Mean Squared Error on Test Data: {loss}\")\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6e21e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 621us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 784us/step\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict(X_train).flatten().tolist()\n",
    "test_pred = model.predict(X_test).flatten().tolist()\n",
    "valid_pred = model.predict(X_valid).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e12152ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "ll = list(itertools.chain(train_pred, test_pred, valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92e0881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06afb697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x1c3d11b4a60>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180975e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
